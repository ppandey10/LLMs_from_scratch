# Large Language Models from Scratch
Building large language models from the ground up — a step-by-step guide to understanding and implementing transformer-based architectures without relying on high-level libraries

This project explores the key concepts and milestones that shaped today’s modern LLMs — by recreating them, piece by piece.

**Implemented Models**

1. `nakliGPT`: A replica of OpenAI's GPT-2 with causal attention (DONE!)
2. `nakliGPT_flash`: A replica of OpenAI's GPT-2 with causal attention and flash attention
3. `nakliGPT_sparse`: A replica of OpenAI's GPT-3 with causal attention and sparse attention
