model:
  vocab_size: 50257          # size of your tokenizer vocab
  embd_dim: 768              # embedding and model dimension
  context_length: 256       # max sequence length
  dropout: 0.1               # dropout for embeddings and layers
  num_heads: 12              # number of attention heads
  num_blocks: 12             # number of transformer blocks
  qkv_bias: False             # whether to use bias in QKV projections

training:
  batch_size: 32
  epochs: 20
  learning_rate: 0.0003
  weight_decay: 0.01
  warmup_steps: 500
  gradient_clip: 1.0

data:
  dataset_path: /home/ge73qip/LLMs/LLMs_from_scratch/the_post_office.txt
  tokenizer_type: custom
  block_size: 1024
  stride: 128

logging:
  log_dir: logs/
  save_model_path: checkpoints/nakliGPT.pt
  log_interval: 50

misc:
  seed: 42
  device: cpu
  # device: cuda
